\chapter{Конструкторский раздел}
Данный раздел содержит описание архитектуры разрабатываемого программного комплекса, описание подсистем, из которых состоит комплекс, а также описание спроектированного алгоритма нормализации входного сигнала.

\section{Общая архитектура разрабатываемого комплекса}
\label{sec:main_arch}

Общая архитектура комплекса представлена на рисунке~\ref{fig:main_arch}.

\begin{figure}[h]
    \center{\includegraphics[width=0.8\textwidth]{include/main_arch_dia.pdf}}
    \caption{Общая архитектура разрабатываемого программного комплекса}
    \label{fig:main_arch}
\end{figure}

В разрабатываемом программном комплексе можно выделить четыре основные подсистемы, перечисленные по уровню абстракции:
\begin{itemize}
\item интерфейс пользователя -- приложение, позволяющее пользователю производить регистрацию и аутентификацию в программном комплексе, реализующее передачу данных с микрофона пользователя на сервер (для сохранения в базе данных и последующей обработки);
\item контроллер сервера -- часть комплекса, выполняющая роль посредника между запросами пользователя и остальными подсистемами. Задача контроллера состоит в обработке запросов от интерфейса пользователя, передаче управления в соответствующие модули программного комплекса, обработке и выдаче результатов обратно в интерфейс;
\item подсистема обучения модели (подсистема регистрации) -- позволяет получить по заданным речевым последовательностям (набору звуковых файлов в базе) персональную модель пользователя, сохранив ее в базе для последующего использования в процессе аутентификации;
\item подсистема голосовой аутентификации -- подсистема, позволяющая определить, принадлежит ли заданная речевая последовательность (в виде набора звуковых файлов в базе) заданному пользователю (персональная модель которого создается на этапе регистрации);
\item подсистема хранения речевых данных пользователей (база данных);
\item библиотека инструментов для голосовой аутентификации -- ядро программного комплекса, реализующее подходы, выбранные в результате обзора существующих решений в разделе~\ref{sec:overview} и предоставляющее интерфейс для подсистем голосовой аутентификации и регистрации.
\end{itemize}

\section{Варианты использования}

На рисунке~\ref{fig:use_cases} представлена диаграмма вариантов использования программного.

\begin{figure}[htp!]
    \center{\includegraphics[width=0.8\textwidth]{include/use_cases_dia.pdf}}
    \caption{Диаграмма вариантов использования программного комплекса}
    \label{fig:use_cases}
\end{figure}

Пользователей программного комплекса можно разделить на две категории:
\begin{itemize}
\item пользователь программного комплекса голосовой аутентификации~-- основной пользователь, использующий
комплекс для прохождения аутентификации на некотором ресурсе. Пользователь имеет
возможность зарегистрироваться в программном комплексе, получив при этом персональную модель
источника речи, обученную на нескольких образцах фразы, которую он должен
повторить в процессе регистрации. После прохождения процесса регистрации
пользователь может использовать программный комплекс для аутентификации на сайте, при этом,
пройдя аутентификацию, пользователь будет обладать теми же правами, которыми он
обладал бы, войдя на ресурс традиционным способом (по паролю);
\item администратор~-- пользователь, наделенный правами на просмотр истории посещений через специализированный интерфейс. Помимо этого, данный тип пользователя может добавлять, редактировать и удалять параметры, влияющие на процесс аутентификации, такие как пути к файлам моделей источников речи, текущая активная модель источника речи для пользователя, величина входного порога для пользователя (определяющий параметр для принятия решения, описанный в разделе~\ref{sec:analytic:decision}). Помимо этого, администратор имеет возможность просматривать журнал, в котором ведется запись основных событий приложени, а также фиксируются возникающие ошибки и предупреждения.
\end{itemize}

\section{Подсистема обучения модели (стадия регистрации)}
\label{sec:construct:enrollment}

При обучении модели пользователю необходимо предоставить несколько образцов,
записей <<кодовой фразы>>, ту же фразу он должен будет впоследствии произнести
для входа на защищаемый ресурс по голосу. Для обзора прохождения процесса регистрации с
точки зрения времени, рассмотрим диаграмму последовательности, представленную на
рисунке~\ref{fig:seq_enrollment}:

\begin{figure}[hp!]
    \center{
        \fontsize{12}{14}\selectfont
        \input{tex_include/seq_enrollment_embedded.tex}
    }
    \caption{Диаграмма последовательности: процесс регистрации в системе}
    \label{fig:seq_enrollment}
\end{figure}

\begin{enumerate}
\item Пользователь инициирует процесс регистрации, интерфейс пользователя
посылает сообщение \Code{register()} контроллеру;
\item Контроллер на стороне сервера создает новую сессию записи, генерируя при
этом уникальный код сессии (\Code{session\_id}), который передается клиенту и
используется в дальнейшем для проверки аутентичности клиента. Состояние
созданной сессии устанавливается в <<Ожидание речевых данных>>;
\item Пользователь производит запись кодовой фразы несколько
раз\footnote{Необходимое количество должно быть определено экспериментально, см.
раздел~\ref{sec:experiment}.}, при этом каждая запись отправляется на сервер и
сохраняется в базе, путь к записи добавляется в данные сессии;
\item После завершения записи фразы, пользователь подтверждает регистрацию, при
этом интерфейс отправляет контроллеру сообщение \Code{confirm()}, после чего
процесс регистрации переходит в состояние <<Обучение модели>> (\Code{started}),
а контроллер ставит соответствующую сессию записи в очередь на обучение, посылая
сообщение \Code{enroll} ядру программного комплекса (подсистеме регистрации);
\item Интерфейс клиента опрашивает контроллер с заданной периодичностью о
состоянии процесса регистрации. Опрос продолжается до тех пор, пока состояние не
окажется одним из тупиковых: <<Обучение завершено>>, <<Ошибка при обучении>> или
<<Обучение прервано>>. На диаграмме показана стереотипная ситуация, при которой
регистрация происходит в штатном режиме (обучение проходит успешно).
\item \label{enum:enroll} Подсистема регистрации, обслуживающая очередь запросов
на обучение модели от контроллера, получает новую заявку и начинает процесс
обучения:
    \begin{enumerate}
        \item Получает контекст сессии (\Code{get\_session\_context}), в котором
        содержатся пути к файлам записей, код сессии и другая информация;
        \item Обращаясь к хранилищу (файловой системе), считывает в память файлы
        записей (\Code{read\_wav});
        \item Вызывает функцию библиотеки инструментов для аутентификации
        \Code{enroll}, передавая параметры обучения, в том числе считанные
        данные.
    \end{enumerate}
\end{enumerate}

Как видно из рисунка~\ref{fig:seq_enrollment} и данного выше описания, сам
процесс обучения происходит отдельно от взаимодействия клиентов и контроллера,
поскольку обучение требует больших вычислительных ресурсов и не может
происходить в том же потоке выполнения, что и клиент-серверное взаимодействие.
В таком случае, процессы обучения рассматриваются подсистемой регистрации как
заявки на обучения и ставятся контроллером в очередь, выполняясь по одиночке и,
тем самым, снижая нагрузку на сервер.

На рисунке~\ref{fig:enrollment_server_sd} представлена диаграмма состояний
для процесса регистрации пользователя в программном комплексе голосовой аутентификации (то
есть процесса обучения новой модели по речевым данным пользователя) с точки
зрения сервера. Переходы между состояниями описаны выше при рассмотрении
диаграммы последовательности.

\begin{figure}
    \center{\includegraphics[width=0.8\textwidth]{include/enrollment_server_sd_dia.pdf}}
    \caption{Диаграмма состояний для процесса обучения модели (сервер)}
    \label{fig:enrollment_server_sd}
\end{figure}

На рисунке~\ref{fig:enrollment_client_sd} представлена диаграмма состояний для
процесса регистрации пользователя в программном комплексе голосовой аутентификации со стороны
пользовательского интерфейса.
\begin{figure}[htp!]
    \center{\includegraphics[width=0.9\textwidth]{include/enrollment_client_sd_dia.pdf}}
    \caption{Диаграмма состояний для процесса обучения модели (клиент)}
    \label{fig:enrollment_client_sd}
\end{figure}


Опишем возможные состояния процесса регистрации с точки зрения интерфейса
пользователя:

\begin{itemize}
\item создан~--- начальное состояние, ожидание начала записи кодовых фраз;
\item запись~--- запись в процессе;
\item останавливается~--- пользователь инициировал остановку записи (или
остановка инициирована автоматически при достижении максимальной длины записи);
\item остановлено~--- запись завершена и в данный момент находится в памяти
клиента. В данном состоянии проверяется, достаточно ли длины записи для отправки
на сервер (запись не должна быть слишком короткой\footnote{Минимальная
допустимая длина записи должна определяться экспериментально.});
\item недостаточно для отправления~--- запись слишком короткая. При этом
возможно возобновление записи (или отмена процесса);
\item отправка на сервер~--- запись достаточной длины, инициируется отправка
данных на сервер. Находясь в данном состоянии, пользовательский интерфейс
ожидает передачи данных и положительного ответа сервера, оповещающего о приеме;
\item ошибка при отправлении~--- данное состояние возникает, если при
отправлении данных на сервер возникла ошибка (соединение оборвалось, сервер не
смог прочитать данные и т.п.);
\item отправлено~--- отправление звуковой записи успешно завершено,
пользовательский интерфейс готов к записи новой фразы. Из этого состояния
возможен переход вновь в состояние записи, отмена процесса регистрации или же
подтверждение (после отправления на сервер минимально допустимого для
регистрации количества записей);
\item подтверждено~--- сервер ответил, оповестив о постановке в очередь запроса
на обучение модели, переход в следующее состояние;
\item обучение в процессе~--- в данном состоянии интерфейс периодически
опрашивает сервер о состоянии процесса обучения (см.
рисунок~\ref{fig:enrollment_server_sd});
\item недостаточно данных для обучения~--- обучение завершилось неудачей из-за
недостаточного количества речевых данных. В данном случае пользователь может
вернуться к записи фразы или отменить процесс регистрации;
\item ошибка при обучении~--- при обучении произошла непредвиденная ошибка, в
данном случае повторное обучение по тем же данным невозможно, пользователю
предложено повторить процесс сначала;
\item обучение завершено~--- обучение успешно завершено, комплекс готов
производить аутентификацию данного пользователя по голосу;
\item отменено~--- в данное состояние возможен переход, если пользователь
отказывается от продолжения процесса регистрации на некотором его этапе.
\end{itemize}

\section{Подсистема голосовой аутентификации}
\label{sec:construct:verification}

В результате завершения процесса регистрации в программном комплексе,
на сервере сохраняется персональная модель для пользователя. При этом для
данного пользователя фиксируется индивидуальный порог вхождения (который
впоследствии может быть скорректирован вручную). Модель пользователя, порог
вхождения и универсальная фоновая модель -- достаточный контекст для проведения
процедуры аутентификации (подробнее см. разделы~\ref{sec:analytic:ubm},
\ref{sec:analytic:decision}).

На рисунке~\ref{fig:seq_verification} представлена диаграмма последовательности
для процесса аутентификации.

\begin{figure}[htp!]
    \center{
        \fontsize{12}{14}\selectfont
        \input{tex_include/seq_verification_embedded.tex}
    }
    \caption{Диаграмма последовательности: процесс аутентификации}
    \label{fig:seq_verification}
\end{figure}

Диаграмма аналогична соответствующей диаграмме для процесса регистрации, поэтому
в дальнейшем описании ограничимся лишь принципиальными различиями между двумя
процессами. Рассмотрим основные этапы процесса аутентификации:

\begin{enumerate}
\item Пользователь инициирует процесс аутентификации, интерфейс пользователя
посылает сообщение \Code{verify()} контроллеру;
\item Контроллер на стороне сервера создает новую сессию записи, генерируя при
этом уникальный код сессии (\Code{session\_id}), состояние
созданной сессии устанавливается в <<Ожидание речевых данных>>;
\item Пользователь производит запись кодовой фразы единожды, после чего запись
отправляется на сервер (\Code{upload(data)}) и сохраняется в базе
(\Code{save\_upload\_wav(filename, data)});
\item После сохранения записанной фразы в базе, контроллер ставит запрос на
аутентификацию в очередь, посылая сообщение \Code{verificate(session\_id)}
подсистеме аутентификации, обслуживающей очередь запросов. Состояние процесса
при этом устанавливается в <<Аутентификация в процессе>>;
\item Интерфейс клиента опрашивает контроллер с заданной периодичностью о
состоянии процесса аутентификации. Опрос продолжается до тех пор, пока состояние
не окажется одним из тупиковых: <<Ошибка в процессе аутентификации>> или
<<Аутентификация успешно завершена>>. В последнем случае, контроллер возвращает
результат аутентификации (булево значение), предварительно завершая все
необходимые действия, по аутентификации и авторизации сессии пользователя в
интернет-портале.
\item \label{enum:verificate} Подсистема аутентификации, обслуживающая очередь
запросов от контроллера, получает новую заявку и начинает процесс аутентификации:
    \begin{enumerate}
        \item Получает контекст сессии (\Code{get\_session\_context}), в котором
        содержатся пути к файлам записей (обычно файл один), код сессии и другая
        информация;
        \item Обращаясь к хранилищу (файловой системе), считывает в память файлы
        записей (\Code{read\_wav}), а также модель источника речи, созданную в
        процессе регистрации и универсальную фоновую модель (\Code{read\_model});
        \item Вызывает функцию библиотеки инструментов для аутентификации
        \Code{verify}, передавая модель пользователя, универсальную фоновую
        модель, порог вхождения и считанные речевые данные.
    \end{enumerate}
\end{enumerate}

На рисунке~\ref{fig:verification_server_sd} представлена диаграмма состояний
для процесса аутентификации пользователя по голосу с точки зрения сервера.
Переходы между состояниями описаны выше при рассмотрении диаграммы
последовательности.

\begin{figure}
    \center{\includegraphics[width=0.8\textwidth]{include/verification_server_sd_dia.pdf}}
    \caption{Диаграмма состояний для процесса аутентификации (сервер)}
    \label{fig:verification_server_sd}
\end{figure}

На рисунке~\ref{fig:verification_client_sd} представлена диаграмма состояний для
процесса аутентификации пользователя по голосу с точки зрения пользовательского
интерфейса.

\begin{figure}[htp!]
    \center{\includegraphics[width=0.9\textwidth]{include/verification_client_sd_dia.pdf}}
    \caption{Диаграмма состояний для процесса аутентификации (клиент)}
    \label{fig:verification_client_sd}
\end{figure}

Состояния и переходы аналогичны соответствующим для процесса регистрации
(рисунок~\ref{fig:enrollment_client_sd}), опишем принципиальные различия:

\begin{enumerate}
\item Конечное состояние в случае успешного завершения процесса аутентификации
распадается на два: <<Успешная аутентификация>> (подразумевается результат
аутентификации, а не успешное завершение самого процесса) и <<В доступе
отказано>>. В первом случае пользователь перенаправляется на ресурс,
который он запросил (или на страницу по-умолчанию).
\item После отправки записи на сервер, интерфейс пользователя сразу переходит в
состояние <<Аутентификация в процессе>> (аналогичное состоянию <<Обучение в
процессе>>), так как для аутентификации требуется только одна запись кодовой
фразы.
\end{enumerate}

\section{Подсистема хранения голосовых данных}
\label{sec:construct:db}

На рисунке~\ref{fig:er_main} представлена диаграмма сущность-связь для разрабатываемой базы данных. Рассмотрим подробнее сущности, представленные на данной диаграмме:

\begin{figure}[ht]
    %\input{tex_include/er_main_embedded.tex}
    %\center{\drawER{0.65}{1.0cm}}
    \center{\includegraphics[width=\textwidth]{include/er_main_tex.pdf}}
    \caption{Диаграмма сущность-связь подсистемы хранения голосовых данных}
    \label{fig:er_main}
\end{figure}

\begin{itemize}
\item \Item{источник речи}. Пользователь программного комплекса голосовой аутентификации, имеющий уникальное \important{имя учетной записи}. Может инициировать \Item{сессию записи}, которая, в свою очередь, может быть использована для процесса обучения персональной \Item{модели источника речи};
\item \Item{модель источника речи}. Персональная модель, параметры которой вычисляются на этапе регистрации, после чего сохраняются в файл для последующего использования, в базе хранится \important{путь к файлу};
\item \Item{универсальная фоновая модель}. Глобальная модель, параметры которой вычисляются заранее, сохраняются в файл для последующего использования. Требуется для процесса аутентификации, в соответствии с описанием в разделе~\ref{sec:analytic:ubm};
\item \Item{сессия записи}. Представляет собой сеанс работы пользователя, предполагающий запись голосовых данных. Сессия записи состоит из одной или нескольких записанных высказываний. Для защиты сессии записи используется уникальный идентификатор сессии, который генерируется на стороне сервера в момент создания сессии и передается клиенту для проверки его последующих запросов на аутентичность. Также записывается \important{IP-адрес пользователя}, с которого совершаются запросы;
\item \Item{верификатор}. Сущность, представляющая собой контекст, необходимый для выдачи решения об аутентификации по заданной речевой последовательности. Верификатор объединяет персональную \Item{модель источника речи}, необходимую для получения меры правдоподобия основной гипотезы, \Item{универсальную фоновую модель}, необходимую для получения меры правдоподобия альтернативной гипотезы, а также хранит значение \important{порога вхождения} для данной модели источника речи ($\Theta_S$ из отношения \ref{eq:decision});
\item \Item{записанная речь}. Фраза, записанная пользователем и переданная на сервер. При этом запись в виде звукового файла сохраняется в хранилище, а в базе хранится путь к этому файлу и дата добавления записи;
\item \Item{процесс обучения}. Сущность представляет собой процесс создания \Item{модели источника речи}. Именно она хранит информацию о \important{состоянии} (см. раздел~\ref{sec:construct:enrollment}), а также о времени создания и завершения. Обучение производится на основе \Item{сессий записи} (обычно на основе одной сессии);
\item \Item{процесс аутентификации}. Сущность представляет собой процесс аутентификации. Объект аутентификации~--- \Item{сессия записи}. Помимо аналогичных атрибутов \important{Состояние}, \important{Дата начала} и \important{Дата конца}, также имеется атрибут \important{Результат}, который содержит булево значение, отражающее факт аутентичности.
\end{itemize}

%Следует отметить, что база данных предназначена для хранения вспомогательной информации, тогда как непосредственно голосовые данные должны храниться 

\section{Ядро комплекса голосовой аутентификации}

Как было указано в разделе~\ref{sec:main_arch}, библиотека инструментов для аутентификации является ядром программного комплекса и расположена на нижнем уровне в его архитектуре. Библиотека представляет собой набор расширяемых классов и функций для полного цикла обработки сигнала (от считывания звукового файла в память и разбора формата до получения массива характерных признаков, готового для обучения или верификации), получения параметров модели источника речи по массиву характерных признаков (обучение модели), а также для собственно верификации по заданной модели и новому массиву характерных признаков.

На рисунке~\ref{fig:inheritance_verispeak} показана диаграмма наследования для классов, составляющих библиотеку инструментов для голосовой аутентификации.

\begin{figure}[ht]
\center{\scalebox{1.0}{\input{tex_include/inheritance_verispeak.tex}}}
\label{fig:inheritance_verispeak}
\caption{Иерархия наследования классов библиотеки голосовой аутентификации}
\end{figure}

Рассмотрим подробнее введенные классы и их атрибуты:

\paragraph{Object}: Базовый класс для всех объектов библиотеки. Группирует остальные классы и позволяет управлять глобальным для всех объектов поведением.

\paragraph{SerializableObject}: класс для объектов, подлежащих сериализации и сохранению в файл. Обладает следующими методами:
\begin{itemize}
\item \Code{serialize()}: возвращает бинарное представление объекта в виде строки;
\item \Code{serializable()}: возвращает контекст объекта, подходящий для передачи в \Code{serialize()} и последующего восстановления объекта;
\item \Code{dump\_to\_file(filename)}: принимает путь к файлу, в который будет сохранен сериализованый объект;
\item \Code{deserialize(loaded\_context)}: преобразует контекст, загруженный из файла, в объект данного класса и возвращает этот объект (классовый метод);
\item \Code{load(filename)}: принимает путь к файлу, в котором сохранен контекст объекта, возвращает объект (классовый метод);
\end{itemize}

\paragraph{Wave}: класс, представляющий звуковую волну как массив амплитуд и значение частоты дискретизации. Используется для преобразования звукового файла одного из поддерживаемых форматов (например, \Code{WAV}) во внутреннее представление для последующей обработки. Методы и атрибуты класса:
\begin{itemize}
\item \Code{read\_file(filename)}: конструктор, считывает данные из звукового файла, возвращает объект класса;
\item \Code{write\_file(filename, format)}: принимает путь к файлу и код формата. Преобразует внутреннее представление звука в файл заданного формата;
\item \Code{waveform}: массив амплитуд (цифровое представление звука);
\item \Code{samplerate}: частота дискретизации сигнала.
\end{itemize}

\paragraph{FramedSpeech}: класс, представляющий сегментированный сигнал. Класс не имеет методов, кроме конструктора, который принимает объект класса \Code{Wave}, длину окна (сегмента) и степень наложения (кадры могут пересекаться друг с другом, таким образом, начало второго кадра совпадает с концом второго и т.~д.). Класс имеет единственный атрибут \Code{frames}, представляющий сегментированный сигнал в виде двумерного массива (каждая строка~-- отдельный сегмент).

\paragraph{FeatureVectors}: базовый класс для классов, представляющих массивы векторов характерных признаков (по которым происходит обучение потомков класса \Code{Codebook} и последующая аутентификация). Все потомки данного класса обязаны определить метод \Code{get\_features()}, возвращающий характерные признаки, по которым обучаются потомки \Code{Codebook}.

\paragraph{MFCCFeatureVectors}: класс, представляющий массив мел-частотных коэффициентов кепстра, описанных в разделе~\ref{sec:analytic:features}. Предназначен для выделения признаков из сегментированного сигнала, на вход конструктор получает объект класса \Code{FramedSpeech} и выделяет из каждого сегмента вектор характерных признаков.

\paragraph{TrainingProcedure}: базовый класс, абстрагирующий процедуру обучения. Описывает метод, который должен быть определен для реализации интерфейса:
\begin{itemize}
\item \Code{train(codebook, feature\_vectors)}: обучение модели (переданной как параметр \Code{codebook}) по массиву векторов характерных признаков (переданному как \Code{feature\_vectors}). Метод возвращает количество итераций (или другую количественную меру длительности обучения).
\end{itemize}

\paragraph{EM}: потомок \Code{TrainingProcedure}, реализующий алгоритм Expectation-Maximization для получения параметров модели по обучающей последовательности (см. раздел~\ref{sec:analytic:em}, \cite{Dempster77EM}). Методы:
\begin{itemize}
\item \Code{expectation()}: соответствует шагу E алгоритма EM;
\item \Code{maximization()}: соответствует шагу M алгоритма EM;
\end{itemize}

\paragraph{MAPAdaptation}: потомок \Code{TrainingProcedure}, реализующий адаптационный алгоритм (Байесово обучение, \important{max a posteriori adaptation}). Адаптационный алгоритм получает получить персональную GMM-модель с помощью универсальной фоновой модели, заранее обученной на большом количестве речевых данных.

\paragraph{FileToFeaturesStack}: класс, представляющий полный цикл необходимых преобразований речи. Позволяет объединить в конвейер объекты классов \Code{Wave}, \Code{FramedSpeech}, \Code{FeatureVectors} и скрыть логику получения характерных признаков речи из звукового файла. Методы и атрибуты:
\begin{itemize}
\item \Code{process(filename)}: запустить выполнение процедур, добавленных в конвейер. Процедуры выполняются по очереди, при этом результат выполненной процедуры передается в качестве аргумента в следующую по списку процедуру. Возвращается результат последней процедуры в списке;
\item \Code{conveyor}: списковая структура, хранящая ссылки на процедуры, которые требуется выполнить по очереди (с помощью метода \Code{process()});
\end{itemize}

\paragraph{CommonStack}: класс, представляющий конвейер преобразований речи, общий для любых характерных признаков (считывание файла и нормализация массива амплитуд). Атрибуты, добавляемые в конвейер:
\begin{itemize}
\item \Code{reader}: процедура, преобразующая звуковой файл во внутреннее представление;
\item \Code{framer}: процедура, сегментирующая внутреннее представление сигнала;
\item \Code{extractor}: процедура, выделяющая характерные признаки из сигнала.
\end{itemize}

\paragraph{CommonMFCCStack}: класс, описывающий цепочку преобразований, применяемых к речевому сигналу для получения характерных признаков в виде \Code{MFCCFeatureVectors}, включая процедуры нормализации.

\paragraph{Codebook}: базовый класс для сущностей, которые могут быть обучены с помощью массива векторов характерных признаков.

\paragraph{GMMBase}: базовый класс для моделей на основе гауссовых смесей (GMM). Конструктор принимает два параметра: $K$~--- количество компонент смеси, $D$~--- размерность компонент (равна размерности вектора характерных признаков). Определяет интерфейс из следующих необходимых методов:
\begin{itemize}
\item \Code{set\_params(w, mu, cov)}: установить параметры модели (параметры, соответственно: веса компонент, двумерный массив векторов математических ожиданий для каждой компоненты и двумерный массив, представляющий матрицы ковариаций компонент);
\item \Code{get\_params(w, mu, cov)}: получить параметры модели, возвращает \Code{w}, \Code{mu}, \Code{cov}, описанные выше;
\item \Code{likelihood(feature\_vectors)}: принимает объект класса \Code{FeatureVectors}, возвращает значение меры правдоподобия (насколько распределение величин в \Code{FeatureVectors} близко к моделируемому);
\item \Code{loglikelihood(feature\_vectors)}: версия \Code{likelihood}, возвращающая логарифм правдоподобия (для облегчения расчетов и борьбы с погрешностью округления);
\end{itemize}

\paragraph{GMM}: реализация интерфейса \Code{GMMBase} для модели с полной матрицей ковариаций у каждой из компонент;

\paragraph{DiagonalGMM}: реализация интерфейса \Code{GMMBase} для модели с диагональной матрицей ковариаций.

Рассмотрим процесс аутентификации с точки зрения потоков данных и функций. На
рисунке~\ref{fig:idef0_main} показана диаграмма функциональных блоков для
процесса верификации.

\begin{figure}
    \center{
    \begin{sideways}
        \includegraphics[width=0.9\textheight]{include/idef0_main_dia.pdf}
    \end{sideways}
    }
    \caption{Диаграмма функциональных блоков A0: аутентификация по голосу}
    \label{fig:idef0_main}
\end{figure}

\begin{enumerate}
\item Прием речевых данных от пользователя. Данный этап не относится напрямую к ядру системы аутентификации и приведен для полноты представления о процессе. В результате данного этапа, подробно описанного в разделе~\ref{sec:construct:verification}, на вход ядру системы подается идентификатор звукового файла, который необходимо проверить на аутентичность;
\item Подготовка входных данных. На данном этапе выполняются все необходимые действия для преобразования звукового файла в массив характерных признаков;
\item Вычисление значения отношения правдоподобия. По данному массиву характерных признаков рассчитывается правдоподобие основной и альтернативной гипотезы (через функцию правдоподобия для модели пользователя и мировой модели (универсальной фоновой модели);
\item Принятие решения об аутентификации. Значение отношения сравнивается с порогом вхождения, на основе сравнения принимается решение об аутентификации пользователя.
\end{enumerate}

Блок подготовки входных данных детализирован на рисунке~\ref{fig:idef0_pre}. Рассмотрим процесс преобразования речи в массив характерных признаков более подробно:

\begin{figure}
    \center{
    \begin{sideways}
    \includegraphics[width=0.9\textheight]{include/idef0_pre_dia.pdf}
    \end{sideways}
    }
    \caption{Диаграмма функциональных блоков A1: подготовка входных данных}
    \label{fig:idef0_pre}
\end{figure}

\begin{enumerate}
\item Считывание звукового файла из хранилища. На этом этапе идентификатор звукового файла (его путь в файловой системе) преобразуется в массив амплитуд;
\item Сегментация речевой последовательности. Массив амплитуд сегментируется и преобразуется в двумерный массив звуковых кадров;
\item Удаление кадров без речи. По завершении получаем двумерных массив кадров, в котором отсутствуют кадры, содержащие тишину (точнее, внешний шум);
\item Вычисление мел-частотных кепстральных коэффициентов. На этом этапе каждый сегмент сигнала преобразуется в массив кепстральных коэффициентов в соответствии с описанием, данным в разделе~\ref{sec:analytic:features}.
\item Расчет дельта-коэффициентов. В соответствии с выражением~\refeq{eq:deltamfcc}, для каждого из коэффициентов MFCC рассчитывается дельта-коэффициент, таким образом, размерность вектора характерных признаков увеличивается вдвое. Это завершающий этап обработки звукового файла, в результате которого получается массив векторов характерных признаков, используемый в процессе обучения и аутентификации.
\end{enumerate}

\section{Алгоритм удаления тишины}
\label{sec:construct:silence_remove}

Как для процесса обучения, так и для процесса аутентификации, для достижения адекватных показателей ошибок необходима нормализация входных речевых данных. Одним из наиболее важных этапов при применении гауссовых смесей\footnote{Причиной важности является тексто-независимость метода. Паузы между словами и предложениями можно рассматривать как характерные признаки источника речи, однако, используемый метод предназначен для моделирования характеристик вокального тракта человека, а не столь высокоуровневых признаков.} является выделение из речевых данных только тех, которые вероятнее содержат речь, а не внешний шум.

Для отделения во входном сигнале речи от условной тишины (постоянного внешнего шума, создаваемого окружением) был разработан специализированный алгоритм, блок-схема которого показана на рисунке~\ref{fig:silence_remove_flowchart}. Алгоритм основан на двух предположениях:

\begin{enumerate}
\item В начала запись всегда есть отрезок без речи. Это предположение равнозначно предположению о том, что человек реагирует на начало записи не мгновенно, а некоторым опозданием. Величина этого отрезка является одним из входных параметров алгоритма;
\item Амплитуда шума имеет распределение, близкое к нормальному. Это предположение исходит из того, что шум окружения -- это случайная величина, зависящая от большого количества факторов, каждый из которых вносит небольшой вклад. Однако, даже если данное предположение неверно, коэффициент доверия остается в пределах, позволяющих практическое использование алгоритма.
\end{enumerate}

\begin{figure}[htp!]
    \center{
    \fontsize{12}{14}\selectfont
    \def\svgwidth{1.2\textwidth}
    \includesvg{silence_remove_flowchart}
    }
    \caption{Блок-схема алгоритма удаления <<тишины>> из входного сигнала}
    \label{fig:silence_remove_flowchart}
\end{figure}

Суть алгоритма проста: предполагая, что первые $\tau_т$ миллисекунд записи -- это внешний шум (условная тишина), рассматриваем амплитуду сигнала как случайную величину и рассчитываем параметры: $\mu$ -- выборочное среднее и $\sigma$ -- выборочное средне-квадратичное отклонение (несмещенная оценка). Тогда, заменяя точные значения выборочными, можно использовать правило, известное в статистике как <<правило трех $\sigma$>> \cite{Pukelsheim1994threesigma}:

\begin{equation}
\textbf{P} \{|X - \mu| > 3\sigma\} < \frac{\sigma^2}{(3\sigma)^2} = \frac{1}{9} = 0.1(1),
\end{equation}

\noindentявляющееся формой второго неравенства Чебышева \cite{BMSTUM17}.

В случае нормального распределения, неравенство превращается в равенство:

\begin{equation}
\textbf{P} \{|X - \mu| < 3\sigma\} = F(\mu + 3\sigma) - F(\mu - 3\sigma) = \Phi(3) - \Phi(-3) ~= 0.9973,
\end{equation}
которое можно неформально интерпретировать так: <<$99.73\%$ значений случайной величины $X$ находятся в интервале $(\mu - 3\sigma, \mu + 3\sigma)$>>.
Входные данные алгоритма описаны в таблице~\ref{tab:unsilence_args}.

\begin{table}[h]
    \caption{Входные данные алгоритма удаления тишины}
    \begin{tabular}{|c|p{13cm}|}
    \hline
    \multicolumn{1}{|c|}{\textbf{Имя переменной}} &
    \multicolumn{1}{|c|}{\textbf{Описание}} \\
    \hline
    $\mathbf{X} = (x_1 x_2 \cdots x_m)$ & Входной сигнал (массив амплитуд) длиной $m$ \\
    $f_{сэмпл}$                         & Частота сэмплирования (дискретизации), Гц \\
    $\tau_{т}$                          & Длина заведомо не содержащего речи отрезка записи (эмпирическая величина), мс\\
    $\tau_{окна}$                       & Ширина окна для сегментирования, мс\\
    $\omega$                            & Параметр, регулирующий чувствительность алгоритма (0 -- алгоритм вернет исходный сигнал, 1 -- максимальная чувствительность), действительное число в интервале $(0, 1]$\\
    \hline
    \end{tabular}
    \label{tab:unsilence_args}
\end{table}

Рассмотрим основные этапы алгоритма:

\begin{enumerate}
\item определяем количество сэмплов $n_т$, в которых предположительно нет речи;
\item определяем выборочное среднее $\mu$ (как среднее арифметическое по первым $n_т$ сэмплам сигнала);
\item определяем выборочную дисперсию $\sigma$ (несмещенную оценку) по первым $n_т$ сэмплам сигнала;
\item определяем длину окна $n_{окна}$ в сэмплах;
\item вызываем процедуру сегментации, получаем двумерный массив $\mathbf{F}$, представляющий собой исходный сигнал, поделенный на сегменты, в каждом из которых $n_{окна}$ сэмплов;
\item создаем изначально пустую списковую структуру $\mathbf{F}_{голос}$, в которую будут добавляться сегменты из $\mathbf{F}$, содержащие голосовые данные;
\item \label{enum:silence:check_segment} для каждого сегмента из $\mathbf{F}$ подсчитываем количество сэмплов $m_{голос}$, для которых выполняется двойное неравенство $\mu\!+\!3\sigma > \mathbf{F}[i][j] > \mu\!-\!3\sigma$;
\item \label{enum:silence:check_segment_end} если доля сэмплов данном сегменте, для которых выполняется предыдущее неравенство, превышает заданную чувствительность $\omega$, помечаем сегмент как содержащий голос, добавляем его в $\mathbf{F}_{голос}$;
\item повторив действия \ref{enum:silence:check_segment})--\ref{enum:silence:check_segment_end}) для каждого сегмента из $\textbf{F}$, возвращаем $\mathbf{F}_{голос}$.
\end{enumerate}

В результате выполнения алгоритма, получается сегментированный сигнал, содержащий только те сегменты, которые удовлетворяют наложенным условиям. После этого сигнал можно снова растянуть и работать как с исходным. Влияние применения разработанного алгоритма на качество аутентификации, а также зависимость качества от параметра $\omega$ рассмотрено в разделе~\ref{sec:experiment}.

\section{Выводы}

В результате выполнения конструкторской части проекта можно сделать следующие выводы:
\begin{enumerate}
\item спроектирована общая архитектура программного комплекса голосовой аутентификации пользователя;
\item спроектирована подсистема регистрации пользователя;
\item спроектирована подсистема аутентификации пользователя;
\item спроектирована схема базы данных и набор сущностей программного комплекса;
\item спроектирована библиотека функций, предназначенных для обработки речи, создания моделей на основе гауссовых смесей и проведения аутентификации. Библиотека разработана с учетом возможности дополнения и расширения функционала;
\item построен алгоритм удаления фрагментов записи, не содержащих речь (удаления тишины).
\end{enumerate}

