\chapter{Аналитическая часть}

В данном разделе будет дано обоснование поставленной задачи, а также обзор современных подходов к решению проблемы голосовой верификации.

\section{Обоснование задачи}


\section{Обзор существующих решений}

Задачи, связанные с определением пользователя по голосу, можно разделить на идентификацию и верификацию. В первом случае, задача состоит в классификации речевого сигнала, при этом каждый класс соответствует одному человеку, зарегистрированному в системе. Во втором случае, задача представляет собой бинарную классификацию. Более формально, задача верификации представляет собой проверку статистической гипотезы, которую можно сформулировать следующим образом \cite{Kinnunen04cohort}. Предположим, что неизвестный произнес высказывание $X$ и представляется пользователем $S$. Две противоположные гипотезы, в таком случае, это

\begin{equation}
\label{eq:hypothesis}
\left\{ 
    \begin{array}{lll}
        H_0 & : & \textrm{высказывание } X \textrm{ произнес } S\\
        H_1 & : & \textrm{высказывание } X \textrm{ произнес \important{не} } S,\\
    \end{array}
\right.
\end{equation}

\noindentи ядро системы верификации должно определить, какую из двух гипотез необходимо принять.

Работу системы верификации говорящего можно разделить на четыре логические стадии:

\begin{enumerate}
\item Нормализация входного речевого сигнала;
\item Выделение характерных признаков;
\item Построение модели говорящего (стадия обучения);
\item Принятие по построенной модели и новой входной последовательности одной из двух гипотез \refeq{eq:hypothesis}.
\end{enumerate}

Среди основных известных проблем в данной области можно выделить следующие (по \cite{Jayanna09overview}):
\begin{itemize}
\item Эффективность современных систем распознавания говорящего уменьшается, если входной сигнал зашумлен из-за окружения, канала передачи и прочих внешний причин. Важным моментом является попытка улучшить сигнал на этой предварительной стадии \cite{Greenwood00suv};
\item Дискриминативная сила моделей теряется при появлении в речи девиантных составляющих, привносимых различными психологическими состояниями говорящего (эмоции, напряжение). Существуют решения, позволяющие компенсировать данные составляющие \cite{Alamo96discriminative};
\item Для биометрической аутентификации человека современным системам требуется значительное количество речевых данных (порядка десятков минут на человека). В таких условиях возрастает и требовательность системы к вычислительным ресурсам. Применимость технологий верификации по голосу увеличится, если удастся разработать технологии, позволяющие получить удовлетворительную производительность с использованием относительно малого количества данных (10-15 секунд). Существующим решением является использование модели гауссовой смеси с использованием так называемой универсальной фоновой модели (UBM -- \important{Universal Background Model}) \cite{Reynolds00speakerverification}.
\end{itemize}

В данном разделе дан обзор основных методов, используемых при построении современных систем такого рода, для каждой из стадий. Раздел заканчивается краткими выводами и предположениями о направлениях исследований в данной сфере.

\subsection{Нормализация входного речевого сигнала}

Важной стадией является предварительная подготовка речевого сигнала до выделения характерных признаков. Основные проблемы в данной области:

\begin{itemize}
\item Удаление <<тишины>> из записи. Самым простым и прямолинейным способом является определение уровня <<энергии сигнала>> и удаление тех кадров, в которых энергия не превышает заданный уровень шума. Другой широко распространенный метод основан на подсчете количества изменений знака амплитуды внутри кадра (\important{zero-crossing rate}). Предложены также комбинации данных методов \cite{Greenwood00suv}, \cite{Bachu04zcr};
\item Подавление шума, вызванного окружением. В данном случае используются методы нормализации, такие как нормализация по средним (\important{mean normalization}). Метод заключается в нахождении среднего значения энергии кадра сигнала и вычитании его из всех сэмплов (чаще данный метод используется после выделения характерных признаков);
\item Частотная развертка. В данном случае производится отсечение частотного спектра сигнала заданным диапазоном, обычно $[300\textrm{Гц}, 4000\textrm{Гц}]$, соответствующего спектру речи человека.
\end{itemize}

В данной работе применены все три метода нормализации.

\subsection{Выделение характерных признаков}

После нормализации сигнала, необходимо выделить признаки, которые характеризуют особенности вокального тракта человека. В сфере обработки сигналов и, в частности, распознавания речи и верификации по голосу, наиболее активное применение нашли так называемые мел-частотные коэффициенты кепстра (\important{MFCC}), отражающие психофизическое восприятие звука человеком. Существуют также исследования, в которых предлагается использование \important{LPCC (Linear prediction coding coefficients)}, вейвлет-преобразований \cite{Medvedev06wavelets}, а также различных их комбинаций. Однако, отдельные исследования показали \cite{Tierney80LPC}, что LPC коэффициенты деградируют, если сигнал зашумлен. В данной работе использованы характерные признаки, основанные на MFCC, как наиболее изученные и подтвердившие свою эффективность в ряде исследований \cite{Jayanna09overview}.

Речевой сигнал изначально представляет собой массив значений амплитуд, взятых с некоторой частотой сэмплирования $f_s$. На рисунке~\ref{fig:waveform} представлен пример такого входного сигнала.

\begin{figure}
\center{\includegraphics[width=0.7\textwidth]{include/waveform_eps.pdf}}
\caption{Пример речевого сигнала (запись слова <<Николай>>)}
\label{fig:waveform}
\end{figure}

Для получения мел-частотных коэффициентов кепстра из входного сигнала, необходимо произвести следующие действия:

\begin{enumerate}
\item Исходный сигнал разбивается на кадры фиксированного размера (10-30мс) с использованием окон Хэмминга;
\item Для каждого кадра выполняется дискретное преобразование Фурье;
\item Полученные коэффициенты возводятся в квадрат и отображаются в мел-пространство с помощью перекрывающихся треугольных окон (\important{mel-scale filterbank});
\item Для полученного массива выполняется дискретное преобразование косинусов (\important{Discrete Cosine Transform}), массив в данном случае рассматривается как сигнал;
\item Коэффициенты преобразования и являются мел-частотными коэффициентами кепстра.
\end{enumerate}

В итоге для каждого кадра из исходного речевого сигнала получаем $N$ коэффициентов $\vec M = (M_1, M_2, \ldots, M_N)$. Первый коэффициент $M_1$ исключается из вектора признаков, так как в действительности представляет собой <<энергию>> сигнала \cite{Reynolds95gmm}. В дополнение к ним, для уменьшения влияния канала передачи сигнала, в современных системах используют \cite{Reynolds95gmm} разницу коэффициентов, находящихся на расстоянии $W$ кадров от текущего ($t$):

\begin{equation}
\Delta \vec M = \vec M_{t+W} - \vec M_{t-W}.
\end{equation}

Последние также называют \important{динамическими} характеристиками сигнала, тогда как мел-частотные коэффициенты являются \important{статическими}.

\subsection{Построение модели говорящего}

Целью данного этапа является построение модели говорящего по специфичным для него векторам характеристических признаков.

В ранних исследованиях в области определения говорящего использовалось прямое сопоставление шаблонов. В сопоставлении шаблонов тестируемый вектор и вектор-образец (обучающий вектор) сравниваются напрямую с помощью некоторой меры похожести, вычисление которой является ключевым при данном подходе. Изначально были использованы Евклидово расстояние или расстояние Махалонобиса. В конце 70-х японские ученые предложили \cite{Sakoe78DTW} концепцию динамической деформации времени (\important{Dynamic Time Warping, DTW}), изначально для распознавания слов. В последствии, Фуруи \cite{Furui81cepstral} предложил использовать данный алгоритм для тексто-зависимой идентификации говорящего. Основным недостатком данного метода считается его высокая требовательность к вычислительным ресурсам по мере увеличения числа характеристических векторов. Однако, в последнее время предложены методы \cite{Lemire09dtw}, позволяющие с помощью эффективного вычисления нижних границ меры уменьшить вычислительную нагрузку алгоритма \important{DTW}.

Следующий шаг в исследовании данной области был сделан в направлении алгоритмов кластеризации. Алгоритм K-средних был использован для получения специфичных для говорящего кодовых векторов для векторного квантования \cite{Soong85VQ}. Был также предложен подход на основе теории нечетких множеств (нечеткое векторное квантование с использованием алгоритма C-средних) \cite{Bezdek78FVQ}, показавший лучшие результаты, по сравнению со стандартным алгоритмом K-средних.

Аппарат скрытых Марковских моделей (\important{Hidden Markov Models, HMM}), успешно применяемый для распознавания речи, был также использован для задачи голосовой идентификации/верификации \cite{Olsson02hmmann}, \cite{Falavigna_comparisonof}. Основным предположением является то, что текущее состояние модели зависит только от предыдущего. В контексте скрытых Марковских моделей, решение задачи верификации требует построения моделей фонем, составляющих речевой сигнал.

В 1995 Рейнольдс предложил модели смесей гауссиан для решения задачи идентификации/верификации по голосу \cite{Reynolds95gmm}. Это наиболее широко распространенный подход, основанный на аппарате математической статистики. Цель данного метода -- моделирование функции плотности распределения векторов характерных признаков, в общем случае нелинейной. В данном методе, сложное распределение моделируется с помощью взвешенной суммы плотностей многомерных нормальных распределений (компонент смеси). Размерность каждого из распределений $D$ совпадает с размерностью вектора характерных признаков. Более формально, плотность распределения описывается следующим выражением:

\begin{equation}
p(\vec x | \lambda) = \sum_{i=1}^K{\omega_i p_i(\vec x)},
\end{equation}

\noindent где $K$ -- количество компонент,\\
$\vec x$ -- вектор характерных признаков,\\
$\omega_i$ -- вес $i$-ой компоненты, веса удовлетворяют условию $\sum_{i=1}^K w_i = 1$,\\
$p_i$ -- плотность распределения $i$-ой компоненты.

Плотность распределения каждой компоненты -- это $D$-мерный гауссиан \cite{BMSTUM16}:

\begin{equation}
\label{eq:mdnormalpdf}
p_i(\vec x) = \frac{1}{(2\pi)^{D/2} |\Sigma_i|^{1/2}}e^{-1/2(\vec x - \vec \mu_i)^T \Sigma_i^{-1} (\vec x - \vec \mu_i)},
\end{equation}

\noindent где $\mu_i$ -- вектор математического ожидания,\\
$\Sigma_i$ -- матрица ковариаций.

Плотность распределения смеси гауссиан полностью описывается параметрами компонент и значениями весов. Параметры модели представим в следующей нотации:

\begin{equation}
\lambda = \{ \omega_i, \vec \mu_i, \Sigma_i  \}, i = \overline{1,K}.
\end{equation}

Каждый пользователь системы представлен собственной моделью $\lambda$.

Модель смеси гауссиан может иметь различные типы, в зависимости от вида матриц ковариаций $\Sigma_i$. В одном случае может использоваться диагональная матрица (все элементы, кроме диагональных, всегда равны нулю, диагональные элементы в данном случае являются значениями средне-квадратичных отклонений $\sigma^2$). В другом случае используются полные матрицы ковариаций. Также возможно использование общей, глобальной матрицы для всех компонент. Эксперименты, проведенные в рамках основополагающей работы по данной теме \cite{Reynolds95gmm}, показали, что лучший результат может быть получен с использованием диагональных матриц ковариаций, отдельных для каждой из компонент. Основным аргументом против использования диагональных матриц является тот факт, что диагональные матрицы подразумевают статистическую независимость коэффициентов вектора характерных признаков. Однако, как показали эксперименты Рейнольдса, смоделировать эту зависимость можно путем введения дополнительных компонент в смесь. Так как вычисления в случае диагональных матриц значительно упрощаются (нет необходимости инвертировать матрицу ковариаций при вычислении \refeq{eq:mdnormalpdf}), автор предлагает использование именно этого варианта.

Для определения параметров модели $\lambda$ существуют несколько методов, наиболее распространенным из которых является метод максимального правдоподобия \cite{BMSTUM17}. Задача метода состоит в нахождении по заданным обучающим данным таких параметров модели, при которых функция правдоподобия модели достигает максимума. Для последовательности из $T$ обучающих векторов $X = \{ \vec x_1, \vec x_2, \ldots \vec x_T \}$, функция правдоподобия может быть записана как

\begin{equation}
\label{eq:likelihood}
p(X | \lambda) = \prod_{t=1}^T p(\vec x_t | \lambda).
\end{equation}

Прямая максимизация выражения \refeq{eq:likelihood} невозможна, так как функция от параметров $\lambda$ нелинейна. Однако, приближенные значения могут быть получены с помощью алгоритма EM (\important{Expectation-Maximization}) \cite{Dempster77EM}.

После этапа обучения, мы имеем готовую модель $\lambda$, соответствующую данной персоне. Теперь, по данной тестовой последовательности, мы можем вычислить функцию правдоподобия $p(S|H_0)$ из выражения \refeq{eq:hypothesis}.

Основным вопросом остается вычисление второй функции, $p(S|H_1)$. По определению, она равна вероятности того, что тестовая последовательность произнесена \important{кем-то, кроме $S$}. В научном сообществе существуют два основных подхода к моделированию альтернативной гипотезы: универсальная фоновая модель (<<мировая>> модель) и модель когортов.

Универсальная фоновая модель (\important{Universal Background Model, UBM}) это модель, цель которой характеризовать всех возможных говорящих <<мира>> во всех возможных контекстах. Данная модель обучается на большом количестве (несколько часов) речевых данных от множества говорящих, сбалансированного по гендерному типу, а также, по возможности, по оборудованию и стандартным условиям. Вычисление правдоподобия $H_1$ в таком случае осуществляется аналогично $H_0$.


Подход на основе смеси гауссиан показывает результат для задачи идентификации на уровне 94.5\% лишь в случае большого количества данных на стадии обучения (60-90 секунд для говорящего) \cite{Reynolds95gmm}. Поэтому были предложены различные методы адаптации, основным из которых можно выделить Байесово обучение \cite{Reynolds00speakerverification}. В данном методе модель говорящего получают путем адаптации параметров <<мировой>> модели по обучающим векторам данного человека.

В данной работе используется подход на основе смеси гауссиан.

\subsection{Принятие решения}

Предположим, что меры правдоподобия обеих гипотез \refeq{eq:hypothesis} известны. Рассмотрим \important{отношение правдоподобия}:

\begin{equation}
\label{eq:lr}
LR_{H_0, H_1} = \frac{p(S|H_0)}{p(S|H_1)},
\end{equation}

\noindent в таком случае, отношение правдоподобия дает оптимальное решение в Баесовом смысле (классификация с минимизацией рисков) \cite{Fukunaga90Intro}. Тогда, процедура принятия решения выглядит следующим образом:

\begin{equation}
\label{eq:decision}
\textrm{Решение } = \left\{ 
    \begin{array}{ll}
        H_0, & LR_{H_0, H_1} > \Theta_{S} \\
        H_1, & LR_{H_0, H_1} \leq \Theta_{S} \\
    \end{array}
\right.
\end{equation}

Ключевым моментом в \refeq{eq:decision} является выбор числа $\Theta_{S}$ (величины порога для пользователя $S$). Величина порога $\Theta_i$ определяется из образцов для обучения таким образом, чтобы обеспечивался необходимый уровень ошибок 1-ого и 2-ого рода. Порог также может быть глобальным для всех пользователей.

\section{Выводы}
